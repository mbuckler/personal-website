+++
abstract = "Hardware support for deep convolutional neural networks (CNNs) is critical to advanced computer vision in mobile and embedded devices. Current designs, however, accelerate generic CNNs; they do not exploit the unique characteristics of real-time vision. We propose to use the temporal redundancy in natural video to avoid unnecessary computation on most frames. A new algorithm, activation motion compensation, detects changes in the visual input and incrementally updates a previously-computed output. The technique takes inspiration from video compression and applies well-known motion estimation techniques to adapt to visual changes. We use an adaptive key frame rate to control the trade-off between efficiency and vision quality as the input changes. We implement the technique in hardware as an extension to existing state-of-the-art CNN accelerator designs. The new unit reduces the average energy per frame by 54.2%, 61.7%, and 87.6% for three CNNs with less than 1% loss in vision accuracy."
abstract_short = ""
authors = [
	"Mark Buckler",
  "Philip Bedoukian",
  "Suren Jayasuriya",
  "Adrian Sampson"]
date = "2018-03-18T19:37:01-04:00"
highlight = true
image_preview = ""
math = false
publication = "International Symposium on Computer Architecture (ISCA)"
publication_short = "ISCA"
publication_types = ["1"]
selected = false
title = "EVAÂ²: Exploiting Temporal Redundancy in Live Computer Vision"
url_pdf_rel = "pdf/isca-2018.pdf"
arXiv = "1803.06312"
doi = ""
url_code = ""
url_dataset = ""
url_project = ""
url_slides = "slides/isca2018.pptx"
url_video = "https://www.youtube.com/watch?v=IX-xfBTcPyo&feature=youtu.be"

[header]
  caption = ""
  image = ""

+++

